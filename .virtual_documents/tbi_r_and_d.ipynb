# !pip install -q datasets transformers torch


from datasets import load_dataset, get_dataset_config_names
import torch
import json
from transformers import pipeline


configs = get_dataset_config_names("ccdv/arxiv-summarization")


configs


ds_section = load_dataset("ccdv/arxiv-summarization", "section", split='train')
ds_doc = load_dataset("ccdv/arxiv-summarization", "document", split='train')


ds_section, ds_doc


# caching a few samples to use in the future to avoid loading from HF every time
sample_sz = 30
section_samples = ds_section.shuffle(seed=42).select(range(sample_sz))
doc_samples = ds_doc.shuffle(seed=42).select(range(sample_sz))

with open("section_samples.json", "w", encoding="utf-8") as f:
    json.dump(section_samples[:], f, indent=2)

with open("doc_samples.json", "w", encoding="utf-8") as f:
    json.dump(doc_samples[:], f, indent=2)








# Load a cached document
with open("section_samples.json", "r", encoding="utf-8") as f:
    section_samples = json.load(f)

section_article = section_samples['article'][0]
section_abstract = section_samples['abstract'][0]

user_prompt = "Summarize the main idea of this paper in one paragraph."


prompt = f"{user_prompt}\n\nDocument:\n{section_article}"





torch.__version__


torch.cuda.is_available()


# Load a lightweight LLM that fits memory/latency constraints
summarizer = pipeline("text2text-generation", model="google/flan-t5-base")





# Run inference
response = summarizer(prompt, max_length=200, do_sample=False)[0]["generated_text"]

print("\nðŸ’¬ User Prompt:", user_prompt)
print("\nðŸ“„ Summary Output:\n", response)
